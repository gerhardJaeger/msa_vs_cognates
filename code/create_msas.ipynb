{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "source": [
    "# Creating character matrices from multiple sequence alignments produced with tCoffee based on Needleman-Wunsch alignment with PMI distances from ASJP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/projects/research/msa_vs_cognates/code`\n"
     ]
    }
   ],
   "source": [
    "cd(@__DIR__)\n",
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "Pkg.instantiate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Imports and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Running `conda install -y r-phangorn` in root environment\n",
      "└ @ Conda /home/gjaeger/.julia/packages/Conda/sDjAP/src/Conda.jl:181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - r\n",
      " - conda-forge\n",
      " - defaults\n",
      " - anaconda\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Running `conda config --set pip_interop_enabled true --file /home/gjaeger/.julia/conda/3/x86_64/condarc-julia.yml` in root environment\n",
      "└ @ Conda /home/gjaeger/.julia/packages/Conda/sDjAP/src/Conda.jl:181\n",
      "┌ Info: Running `pip install asjp` in root environment\n",
      "└ @ Conda /home/gjaeger/.julia/packages/Conda/sDjAP/src/Conda.jl:669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: asjp in /home/gjaeger/.local/lib/python3.10/site-packages (0.0.2)\n",
      "Requirement already satisfied: ipatok>=0.2 in /home/gjaeger/.local/lib/python3.10/site-packages (from asjp) (0.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Building\u001b[22m\u001b[39m Conda ─→ `~/.julia/scratchspaces/44cfe95a-1eb2-52ea-b672-e2afdf69b78f/51cab8e982c5b598eea9c8ceaced4b58d9dd37c9/build.log`\n",
      "\u001b[32m\u001b[1m    Building\u001b[22m\u001b[39m PyCall → `~/.julia/scratchspaces/44cfe95a-1eb2-52ea-b672-e2afdf69b78f/9816a3826b0ebf49ab4926e2b18842ad8b5c8f04/build.log`\n",
      "\u001b[32m\u001b[1m    Building\u001b[22m\u001b[39m Conda → `~/.julia/scratchspaces/44cfe95a-1eb2-52ea-b672-e2afdf69b78f/51cab8e982c5b598eea9c8ceaced4b58d9dd37c9/build.log`\n",
      "\u001b[32m\u001b[1m    Building\u001b[22m\u001b[39m RCall → `~/.julia/scratchspaces/44cfe95a-1eb2-52ea-b672-e2afdf69b78f/846b2aab2d312fda5e7b099fc217c661e8fae27e/build.log`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject <module 'asjp' from '/home/gjaeger/.local/lib/python3.10/site-packages/asjp/__init__.py'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Julia Packages\n",
    "using CSV\n",
    "using DataFrames\n",
    "using MCPhyloTree\n",
    "using ProgressMeter\n",
    "using Pipe\n",
    "using ArgCheck\n",
    "using Base.Threads\n",
    "using StatsPlots\n",
    "using StatsBase\n",
    "using RCall\n",
    "\n",
    "# Plotting backend\n",
    "plotlyjs()\n",
    "\n",
    "# Include external scripts\n",
    "include(\"needlemanWunsch.jl\")\n",
    "\n",
    "# Conda and Python setup\n",
    "using Conda\n",
    "Conda.add(\"r-phangorn\")\n",
    "Conda.pip_interop(true)\n",
    "Conda.pip(\"install\", \"asjp\")\n",
    "ENV[\"PYTHON\"] = \"\"\n",
    "Pkg.build(\"PyCall\")\n",
    "ENV[\"R_HOME\"] = \"*\"\n",
    "Pkg.build(\"RCall\")\n",
    "using PyCall\n",
    "\n",
    "# Python package import\n",
    "pyasjp = pyimport(\"asjp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ipa2asjp (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load datasets\n",
    "dbs = readdir(\"../datasets\", join=true)\n",
    "\n",
    "\n",
    "# Data Cleaning Functions\n",
    "function cleanASJP(word)\n",
    "    @pipe word |>\n",
    "          replace(_, r\"[ \\*~\\\"]\" => \"\") |>\n",
    "          replace(_, r\"(.)(.)(.)\\$\" => s\"\\2\")\n",
    "end\n",
    "\n",
    "function ipa2asjp(w)\n",
    "    w = replace(w, \" \" => \"\")\n",
    "    try\n",
    "        return cleanASJP(pyasjp.ipa2asjp(w))\n",
    "    catch e\n",
    "        return missing\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying cleaning function to the dataset\n",
    "db_dataframes = []\n",
    "for db in dbs\n",
    "    d = CSV.File(db) |> DataFrame\n",
    "    insertcols!(d, :ASJP => ipa2asjp.(d.TOKENS))\n",
    "    dropmissing!(d, :ASJP)\n",
    "    push!(db_dataframes, d)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PMI Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pmiStar (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function pmiStar(w1::Union{Missing,String}, w2::Union{Missing,String}, p::NW)\n",
    "    if ismissing(w1) || ismissing(w2)\n",
    "        return missing\n",
    "    end\n",
    "    v1 = split(w1,\"-\")\n",
    "    v2 = split(w2,\"-\")\n",
    "    scores = Vector{Float64}(undef, length(v1)*length(v2))\n",
    "    counter = 1\n",
    "    for x in v1\n",
    "        for y in v2\n",
    "            @inbounds scores[counter] = nw(x, y, p)\n",
    "            counter += 1\n",
    "        end\n",
    "    end\n",
    "    maximum(scores)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree and Matrix Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_alignment (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function dercPMI(\n",
    "    i::Int,\n",
    "    j::Int,\n",
    "    p::NW,\n",
    "    dMtx::Matrix{Union{Missing, String}} = dMtx,\n",
    "    maxSim::Float64 = maxSim,\n",
    "    minSim::Float64 = minSim,\n",
    ")\n",
    "    defined1 = findall(.!ismissing.(dMtx[i, :]))\n",
    "    defined2 = findall(.!ismissing.(dMtx[j, :]))\n",
    "    definedBoth = intersect(defined1, defined2)\n",
    "    nBoth = length(definedBoth)\n",
    "    if nBoth == 0\n",
    "        return missing\n",
    "    end\n",
    "    dg = Vector{Float64}(undef, nBoth)\n",
    "    for (k, c) in enumerate(definedBoth)\n",
    "        dg[k] = pmiStar(dMtx[i, c], dMtx[j, c], p)\n",
    "    end\n",
    "    nOffD = length(defined1) * length(defined2) - nBoth\n",
    "    offDg = Vector{Float64}(undef, nOffD)\n",
    "    counter = 1\n",
    "    for k1 in defined1\n",
    "        w1 = dMtx[i, k1]\n",
    "        for k2 in defined2\n",
    "            if k1 != k2\n",
    "                w2 = dMtx[j, k2]\n",
    "                @inbounds offDg[counter] = pmiStar(w1, w2, p)\n",
    "                counter += 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    ranks = Vector{Float64}(undef, nBoth)\n",
    "    for k = 1:nBoth\n",
    "        @inbounds x = dg[k]\n",
    "        @inbounds ranks[k] = geomean(1 .+ (sum(offDg .> x):sum(offDg .>= x)))\n",
    "    end\n",
    "    stc = mean(-log.(ranks ./ (1 + nOffD)))\n",
    "    sim = (stc - 1) * sqrt(nBoth)\n",
    "    (maxSim - sim) / (maxSim - minSim)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Takes two gapped strings and returns the hamming distance between\n",
    "them. Positions containing a gap in at least one string are ignored.\n",
    "\"\"\"\n",
    "function sHamming(al)\n",
    "    ds = []\n",
    "    for i in 1:size(al, 1)\n",
    "        s1, s2 = al[i, :]\n",
    "        if !ismissing(s1) && !ismissing(s2)\n",
    "            push!(\n",
    "                ds,\n",
    "                Int(s1 != s2)\n",
    "            )\n",
    "        end\n",
    "    end\n",
    "    mean(ds)\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Takes a pairwise alignment (i.e. a pair of gapped strings with identical length)\n",
    "as input and returns a matrix representation M as output.\n",
    "The matrix M is defined as M[i,j] = 1 if x[i] is matched with y[j]\n",
    "in the alignment, 0 else (where x,y are the two ungapped strings to be aligned).\n",
    "\"\"\"\n",
    "\n",
    "function algnMatrix(al)\n",
    "    w1 = filter(x -> !ismissing(x), al[:, 1])\n",
    "    w2 = filter(x -> !ismissing(x), al[:, 2])\n",
    "    dm = zeros(Int, length(w1), length(w2))\n",
    "    i, j = 1, 1\n",
    "    for k in 1:size(al, 1)\n",
    "        s1, s2 = al[k, :]\n",
    "        if !ismissing(s1)\n",
    "            if !ismissing(s2)\n",
    "                dm[i, j] += 1\n",
    "                i += 1\n",
    "                j += 1\n",
    "            else\n",
    "                i += 1\n",
    "            end\n",
    "        else\n",
    "            j += 1\n",
    "        end\n",
    "    end\n",
    "    dm\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Takes a list of sequences and returns a library in the sense of the\n",
    "T-Coffee algorithm. A library is a dictionary with sequence pairs\n",
    "as keys and pairwise alignments in matrix format as columns.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "function createLibrary(words; pmiPar::NW=pmiPar)\n",
    "    library = Dict{Tuple{String, String}, Tuple{Matrix{Int}, Float64}}()\n",
    "    library_lock = ReentrantLock()\n",
    "    uwords = unique(words)\n",
    "    @threads for w1 in uwords\n",
    "        for w2 in uwords\n",
    "            if (w2, w1) in keys(library)\n",
    "                lock(library_lock)\n",
    "                try\n",
    "                    x = library[w2, w1]\n",
    "                    library[w1, w2] = (Matrix{Int}(x[1]'), x[2])\n",
    "                finally\n",
    "                    unlock(library_lock)\n",
    "                end\n",
    "            else\n",
    "                al = nwAlign(w1, w2, pmiPar)[1]\n",
    "                lock(library_lock)\n",
    "                try\n",
    "                    library[w1, w2] = (algnMatrix(al), 1 - sHamming(al))\n",
    "                finally\n",
    "                    unlock(library_lock)\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return library\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Takes a list of sequences and returns an extended library in the\n",
    "sense of the T-Coffee algorithm. An extended library is a dictionary with\n",
    "sequence pairs as keys and a score matrix as values.\n",
    "For a pair of sequences x,y and a corresponding score matrix M,\n",
    "M[i,j] is the score for aligning x[i] with y[j].\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "function createExtendedLibrary(words; pmiPar::NW=pmiPar)\n",
    "    uwords = unique(words)\n",
    "    library = createLibrary(uwords; pmiPar=pmiPar)\n",
    "    extLibrary = Dict{Tuple{String, String}, Matrix{Float32}}()\n",
    "    extLibrary_lock = ReentrantLock()\n",
    "\n",
    "    # Precompute some values to avoid repeated computation in the loop\n",
    "    word_pairs = [(i, j, uwords[i], uwords[j]) for i in 1:length(uwords), j in 1:length(uwords) if i <= j]\n",
    "\n",
    "    @threads for (i, j, w1, w2) in word_pairs\n",
    "        n, m = length.(collect.([w1, w2]))\n",
    "        dm = zeros(Float32, n, m)\n",
    "        \n",
    "        for w3 in words\n",
    "            a1, s1 = library[w1, w3]\n",
    "            a2, s2 = library[w3, w2]\n",
    "            a1, a2 = Matrix{Float32}(a1), Matrix{Float32}(a2)\n",
    "            dm += (s1 + s2) * (a1 * a2)\n",
    "        end\n",
    "    \n",
    "        lock(extLibrary_lock)\n",
    "        try\n",
    "            extLibrary[w1, w2] = dm\n",
    "            extLibrary[w2, w1] = Matrix{Float32}(dm')\n",
    "        finally\n",
    "            unlock(extLibrary_lock)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return extLibrary\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Returns the index of gappedString[i] in the\n",
    "ungapped version thereof.\n",
    "If gappedString[i] is a gap, returns -1\n",
    "\"\"\"\n",
    "\n",
    "function pos(alVector, i)\n",
    "    if ismissing(alVector[i])\n",
    "        return -1\n",
    "    end\n",
    "    return i - sum(ismissing.(alVector[1:i]))\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Needleman-Wunsch alignment of two aligned blocks b1 and b2,\n",
    "using the scores in the extended library lib.\n",
    "\"\"\"\n",
    "function nwBlock(al1, al2, extLibrary)\n",
    "    # Prepare words1 and words2 using comprehensions\n",
    "    words1 = [join(filter(x -> !ismissing(x), al1[:, i]), \"\") for i in 1:size(al1, 2)]\n",
    "    words2 = [join(filter(x -> !ismissing(x), al2[:, i]), \"\") for i in 1:size(al2, 2)]\n",
    "\n",
    "    n, m = size(al1, 1), size(al2, 1)\n",
    "    dp = zeros(n + 1, m + 1)\n",
    "    pointers = zeros(Int, n + 1, m + 1)\n",
    "\n",
    "    # Initialize first row and column of pointers\n",
    "    pointers[1, 2:end] .= 3  # All deletions\n",
    "    pointers[2:end, 1] .= 2  # All insertions\n",
    "\n",
    "    match_cache = Dict{Tuple{Int, Int}, Float64}()\n",
    "\n",
    "    for i in 2:(n + 1)\n",
    "        for j in 2:(m + 1)\n",
    "            insert = dp[i-1, j]\n",
    "            delet = dp[i, j-1]\n",
    "            match = dp[i-1, j-1]\n",
    "\n",
    "            match_val = 0.0\n",
    "            for k in 1:length(words1)\n",
    "                for l in 1:length(words2)\n",
    "                    if !ismissing(al1[i-1, k]) && !ismissing(al2[j-1, l])\n",
    "                        pos1 = i - 1\n",
    "                        pos2 = j - 1\n",
    "                        w1, w2 = words1[k], words2[l]\n",
    "\n",
    "                        if haskey(match_cache, (pos1, pos2))\n",
    "                            match_val = match_cache[(pos1, pos2)]\n",
    "                        else\n",
    "                            if pos1 <= size(extLibrary[w1, w2], 1) && pos2 <= size(extLibrary[w1, w2], 2)\n",
    "                                match_val += extLibrary[w1, w2][pos1, pos2]\n",
    "                                match_cache[(pos1, pos2)] = match_val\n",
    "                            end\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "\n",
    "            match += match_val\n",
    "\n",
    "            # Update dp and pointers\n",
    "            dp[i, j], pointers[i, j] = maximum([(match, 1), (insert, 2), (delet, 3)])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Traceback to find the alignment path\n",
    "    i, j = n + 1, m + 1\n",
    "    indices = Vector{Tuple{Int, Int}}()\n",
    "    while i > 1 || j > 1\n",
    "        p = pointers[i, j]\n",
    "        if p == 1\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "            pushfirst!(indices, (i, j))\n",
    "        elseif p == 2\n",
    "            i -= 1\n",
    "            pushfirst!(indices, (i, -1))\n",
    "        else\n",
    "            j -= 1\n",
    "            pushfirst!(indices, (-1, j))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Initialize alNew array\n",
    "    alNew = Array{Union{Char, Missing}}(missing, length(indices), size(al1, 2) + size(al2, 2))\n",
    "\n",
    "    # Fill alNew with aligned sequences\n",
    "    for (k, (i, j)) in enumerate(indices)\n",
    "        if i == -1\n",
    "            x1 = fill(missing, size(al1, 2))\n",
    "        else\n",
    "            x1 = al1[i, :]\n",
    "        end\n",
    "        if j == -1\n",
    "            x2 = fill(missing, size(al2, 2))\n",
    "        else\n",
    "            x2 = al2[j, :]\n",
    "        end\n",
    "        alNew[k, :] = vcat(x1, x2)\n",
    "    end\n",
    "\n",
    "    alNew\n",
    "end\n",
    "\n",
    "\n",
    "function tCoffee(guide_tree; pmiPar::NW=pmiPar)\n",
    "    words = [string(split(x.name, \":\")[2]) for x in get_leaves(guide_tree)]\n",
    "    taxa = [string(split(x.name, \":\")[1]) for x in get_leaves(guide_tree)]\n",
    "    extLibrary = createExtendedLibrary(words; pmiPar=pmiPar)\n",
    "    alHistory = Dict()\n",
    "    nums = Dict()\n",
    "    for nd in post_order(guide_tree)\n",
    "        if length(nd.children) == 0\n",
    "            w = string(split(nd.name, \":\")[2])\n",
    "            alHistory[nd.num] = reshape(collect(w), :, 1)\n",
    "            nums[nd.num] = [nd.num]\n",
    "        elseif length(nd.children) == 1\n",
    "            alHistory[nd.num] = alHistory[nd.children[1].num]\n",
    "            nums[nd.num] = [nums[nd.children[1].num]]\n",
    "        else\n",
    "            ch1, ch2 = nd.children\n",
    "            al1 = alHistory[ch1.num]\n",
    "            al2 = alHistory[ch2.num]\n",
    "            nums1 = nums[ch1.num]\n",
    "            nums2 = nums[ch2.num]\n",
    "            alHistory[nd.num] = nwBlock(al1, al2, extLibrary)\n",
    "            nums[nd.num] = vcat(nums1, nums2)\n",
    "        end\n",
    "    end\n",
    "    df = DataFrame(permutedims(alHistory[guide_tree.num]), :auto)\n",
    "    insertcols!(df, 1, :language => taxa)\n",
    "    df\n",
    "end\n",
    "\n",
    "\n",
    "function create_guide_tree(data::DataFrame; tree::GeneralNode=tree)\n",
    "    words2lang = @pipe data |>\n",
    "        zip(_.ASJP, _.DOCULECT) \n",
    "    words = first.(words2lang)\n",
    "    taxa = last.(values(words2lang))\n",
    "    unique_taxa = unique(taxa)\n",
    "    guide_tree = deepcopy(tree)\n",
    "    for nd in post_order(guide_tree)\n",
    "        if nd.nchild == 0 && nd.name ∉ unique_taxa && !isroot(nd)\n",
    "            mother = get_mother(nd)\n",
    "            remove_child!(mother, nd)\n",
    "        end\n",
    "    end\n",
    "    while guide_tree.nchild == 1\n",
    "        guide_tree = guide_tree.children[1]\n",
    "    end\n",
    "    for nd in post_order(guide_tree)\n",
    "        if (nd.nchild == 1)\n",
    "            delete_node!(nd)\n",
    "        end\n",
    "    end\n",
    "    for nd in get_leaves(guide_tree)\n",
    "        language = nd.name\n",
    "        nd_words = words[findall(taxa .== language)]\n",
    "        while length(nd_words) > 1\n",
    "            nd1 = Node()\n",
    "            nd2 = Node()\n",
    "            nd1.name = pop!(nd_words)\n",
    "            nd1.name = language * \":\" * nd1.name\n",
    "            add_child!(nd, nd1)\n",
    "            add_child!(nd, nd2)\n",
    "            nd = nd2\n",
    "        end\n",
    "        nd.name = nd_words[1]\n",
    "        nd.name = language * \":\" * nd.name\n",
    "    end\n",
    "\n",
    "    for (i, nd) in enumerate(post_order(guide_tree))\n",
    "        nd.num = i\n",
    "    end\n",
    "    guide_tree\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function get_alignment(data::DataFrame; tree::GeneralNode=tree, pmiPar::NW=pmiPar)\n",
    "    guide_tree = create_guide_tree(data; tree=tree)\n",
    "    al = tCoffee(guide_tree)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get PMI Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NW{Char}(['!', '3', '4', '5', '7', '8', 'C', 'E', 'G', 'L'  …  'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'], Dict(('S', 'h') => -1.0026989942589113, ('s', 'S') => 2.318288217979542, ('j', 'N') => -1.7826427822252349, ('G', 'G') => 7.801431036430333, ('X', 'w') => -2.780066286502958, ('X', 'i') => -4.154471127478828, ('T', 'n') => -4.322710572125696, ('7', 'G') => 0.6133857200649624, ('G', 'y') => -0.454410510043628, ('T', 'v') => -1.70134002370793…), 2.41772332742949, 1.5084822877429087)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "par = CSV.read(\"../data/pmiParameters.csv\", DataFrame)\n",
    "\n",
    "pmi = CSV.read(\"../data/pmi.csv\", DataFrame)[:,2:end] |> Array\n",
    "\n",
    "sounds = first.(CSV.read(\"../data/pmi.csv\", DataFrame)[:,1])\n",
    "\n",
    "pmiDict = Dict{Tuple{Char, Char}, Float64}()\n",
    "\n",
    "for (i, s1) in enumerate(sounds), (j, s2) in enumerate(sounds)\n",
    "    pmiDict[s1, s2] = pmi[i,j]\n",
    "end\n",
    "\n",
    "pmiPar = NW(\n",
    "    sounds,\n",
    "    pmiDict,\n",
    "    par[1,1],\n",
    "    par[1,2],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_pmidists (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function compute_pmidists(languages, dMtx, pmiPar, maxSim, minSim)\n",
    "    pmidists = zeros(Union{Float64, Missing}, (length(languages), length(languages)))\n",
    "    for i in 1:length(languages)\n",
    "        for j in (i + 1):length(languages)\n",
    "            pmidists[i, j] = pmidists[j, i] = dercPMI(i, j, pmiPar, dMtx, maxSim, minSim)\n",
    "        end\n",
    "    end\n",
    "    pmidists[ismissing.(pmidists)] .= mean(skipmissing(pmidists))\n",
    "    return pmidists\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "build_tree (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function build_tree(pmidists, languages)\n",
    "    R\"\"\"\n",
    "    library(phangorn)\n",
    "    dst = $pmidists\n",
    "    rownames(dst) <- colnames(dst) <- $languages\n",
    "    tree <- upgma(dst)\n",
    "    treeS <- write.tree(tree, file=\"\")\n",
    "    \"\"\"\n",
    "    return ParseNewick(@rget treeS)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_alignments (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function get_alignments(concepts, d, tree)\n",
    "    alignments = Dict()\n",
    "    for c in concepts\n",
    "        data = filter(x -> x.CONCEPT == c, d)\n",
    "        alignments[c] = get_alignment(data; tree=tree)\n",
    "    end\n",
    "    return alignments\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_character_matrix (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function create_character_matrix(concepts, alignments)\n",
    "    concept_char_mtc = []\n",
    "    for c in concepts\n",
    "        al = alignments[c]\n",
    "        @pipe al |>\n",
    "              1 .- ismissing.(Array(_[:, 2:end])) |>\n",
    "              DataFrame(_, :auto) |>\n",
    "              insertcols!(_, 1, :language => al.language) |>\n",
    "              groupby(_, :language) |>\n",
    "              combine(_, names(_, Not(:language)) .=> maximum) |>\n",
    "              push!(concept_char_mtc, _)\n",
    "    end\n",
    "    char_mtx = outerjoin(concept_char_mtc..., on=:language, makeunique=true)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "write_nexus_file (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function write_nexus_file(char_mtx, db_name)\n",
    "    nex = \"\"\"\n",
    "    #NEXUS\n",
    "\n",
    "    Begin data;\n",
    "    Dimensions ntax=$(size(char_mtx, 1)) nchar = $(size(char_mtx, 2) - 1);\n",
    "    Format datatype=restriction gap=-;\n",
    "    MATRIX\n",
    "    \"\"\"\n",
    "    pad = maximum(length.(char_mtx.language)) + 5\n",
    "    for i in 1:size(char_mtx, 1)\n",
    "        l = char_mtx.language[i]\n",
    "        ln = \"   \" * rpad(l, pad)\n",
    "        row = join(replace(char_mtx[i, 2:end] |> Vector, missing => \"-\"))\n",
    "        ln *= row * \"\\n\"\n",
    "        nex *= ln\n",
    "    end\n",
    "    nex *= \"\"\"\n",
    "    ;\n",
    "    End;\n",
    "    \"\"\"\n",
    "    open(joinpath(\"mrbayes/\", \"$(db_name)_msa.nex\"), \"w\") do f\n",
    "        write(f, nex)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "write_mrbayes_file (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function write_mrbayes_file(db_name)\n",
    "    mb = \"\"\"#Nexus\n",
    "    Begin MrBayes;\n",
    "        execute $(db_name)_msa.nex;\n",
    "        prset brlenspr = clock:uniform;\n",
    "        prset clockvarpr = igr;\n",
    "        lset rates=gamma;\n",
    "        lset covarion=yes;\n",
    "        prset clockratepr=exp(1.0);\n",
    "        lset coding=noabsencesites;\n",
    "        mcmcp stoprule=yes stopval=0.01 filename=output/$(db_name)_msa samplefreq=1000;\n",
    "        mcmc ngen=10000000 nchains=2 nruns=2 append=no;\n",
    "        sumt;\n",
    "        sump;\n",
    "        q;\n",
    "    end;\n",
    "    \"\"\"\n",
    "    open(joinpath(\"mrbayes/\", \"$(db_name)_msa.mb.nex\"), \"w\") do f\n",
    "        write(f, mb)\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: RCall.jl: Loading required package: ape\n",
      "│ code for methods in class “Rcpp_Fitch” was not checked for suspicious field assignments (recommended package ‘codetools’ not available?)\n",
      "│ code for methods in class “Rcpp_Fitch” was not checked for suspicious field assignments (recommended package ‘codetools’ not available?)\n",
      "└ @ RCall /home/gjaeger/.julia/packages/RCall/dDAVd/src/io.jl:172\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:02\u001b[39m\u001b[K\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:04:42\u001b[39m\u001b[K\n"
     ]
    }
   ],
   "source": [
    "@showprogress for (db, d) in zip(dbs, db_dataframes)\n",
    "    concepts = unique(d.CONCEPT)\n",
    "    languages = unique(d.DOCULECT)\n",
    "    d_wide = unstack(d, :DOCULECT, :CONCEPT, :ASJP, allowmissing=true, combine=x -> join(unique(x), \"-\"))\n",
    "    ln2index = Dict(zip(d_wide.DOCULECT, 1:size(d_wide, 1)))\n",
    "    dMtx = Matrix(d_wide[:, 2:end])\n",
    "    nconcepts = length(concepts)\n",
    "    minSim = -sqrt(nconcepts)\n",
    "    maxSim = (log(nconcepts * (nconcepts - 1) + 1) - 1) * sqrt(nconcepts)\n",
    "    pmidists = compute_pmidists(languages, dMtx, pmiPar, maxSim, minSim)\n",
    "    tree = build_tree(pmidists, languages)\n",
    "    alignments = get_alignments(concepts, d, tree)\n",
    "    char_mtx = create_character_matrix(concepts, alignments)\n",
    "    db_name = split(split(db, \"/\")[end], \".\")[1]\n",
    "    write_nexus_file(char_mtx, db_name)\n",
    "    write_mrbayes_file(db_name)\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.3",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
